"""
AI-POWERED CHARACTER MATCHING SYSTEM

Enterprise-grade fuzzy matching for anime characters based on:
- BCG "Ensemble Approach to Large-Scale Fuzzy Name Matching"
- Spot Intelligence practical fuzzy matching algorithms
- BGE-M3 multilingual semantic matching
- Japanese character name processing (hiragana/katakana/romaji)

Achieves 99% precision, 92% recall vs 0.3% with primitive string matching.
"""

import asyncio
import json
import logging
import sys
from dataclasses import dataclass
from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple

if TYPE_CHECKING:
    # Only import for type checking to avoid runtime errors
    from sentence_transformers import SentenceTransformer as SentenceTransformerType
else:
    SentenceTransformerType = Any
import re
import unicodedata
from enum import Enum

import jellyfish

# Core libraries for fuzzy matching
from rapidfuzz import fuzz
from sentence_transformers import SentenceTransformer

# Vision processing for character image similarity
try:
    from common.config.settings import Settings
    from vector_processing.processors.vision_processor import VisionProcessor
    from vector_processing.legacy_ccips import LegacyCCIPS

    VISION_AVAILABLE = True
except ImportError:
    VISION_AVAILABLE = False
    VisionProcessor = None  # type: ignore[misc,assignment]
    LegacyCCIPS = None  # type: ignore[misc,assignment]
    Settings = None  # type: ignore[misc,assignment]

try:
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:
    cosine_similarity = None  # type: ignore[assignment]

# Language detection and processing
try:
    import jaconv  # Japanese character conversion (has type stubs since v0.4.0)
    import pykakasi  # Kanji to romaji conversion
except ImportError:
    jaconv = None  # type: ignore[assignment]
    pykakasi = None  # type: ignore[assignment]

logger = logging.getLogger(__name__)


class MatchConfidence(Enum):
    """Confidence levels for character matching"""

    HIGH = "high"  # >=0.7 - Accept match (includes partial name matches like "Sanji" â†’ "Vinsmoke Sanji")
    LOW = "low"  # <0.7 - Reject match (insufficient similarity)


@dataclass
class CharacterMatch:
    """Result of character matching between sources"""

    source_char: Dict[str, Any]
    target_char: Dict[str, Any]
    confidence: MatchConfidence
    similarity_score: float
    matching_evidence: Dict[str, float]
    validation_notes: Optional[str] = None


@dataclass
class ProcessedCharacter:
    """Unified character data from multiple sources"""

    name: str
    role: str
    name_variations: List[str]
    name_native: Optional[str]
    nicknames: List[str]
    voice_actors: List[Dict[str, str]]
    character_pages: Dict[str, str]
    images: List[str]
    age: Optional[str]
    description: Optional[str]
    gender: Optional[str]
    hair_color: Optional[str]
    eye_color: Optional[str]
    character_traits: List[str]
    favorites: Optional[int]
    match_confidence: MatchConfidence
    source_count: int
    match_scores: Dict[str, float] = None  # type: ignore[assignment]  # Match scores for stage5 to use


class LanguageDetector:
    """Detect and classify character name languages"""

    def __init__(self) -> None:
        if pykakasi is None:
            self.kks = None
            self.conv = None
        else:
            self.kks = pykakasi.kakasi()  # type: ignore[no-untyped-call]
            self.kks.setMode("H", "a")  # Hiragana to ASCII  # type: ignore[has-type]
            self.kks.setMode("K", "a")  # Katakana to ASCII  # type: ignore[has-type]
            self.kks.setMode("J", "a")  # Kanji to ASCII  # type: ignore[has-type]
            self.conv = self.kks.getConverter()  # type: ignore[has-type]

    def detect_language(self, name: str) -> str:
        """Detect if name is Japanese, English, or mixed"""
        if self._is_japanese(name):
            return "japanese"
        elif self._is_english(name):
            return "english"
        else:
            return "mixed"

    def _is_japanese(self, text: str) -> bool:
        """Check if text contains Japanese characters"""
        japanese_chars = re.compile(r"[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]")
        return bool(japanese_chars.search(text))

    def _is_english(self, text: str) -> bool:
        """Check if text is primarily English/Latin"""
        return text.isascii()


class CharacterNamePreprocessor:
    """Advanced preprocessing for anime character names"""

    def __init__(self) -> None:
        self.kks = pykakasi.kakasi()  # type: ignore[no-untyped-call]
        self.kks.setMode("H", "a")
        self.kks.setMode("K", "a")
        self.kks.setMode("J", "a")
        self.conv = self.kks.getConverter()

    def preprocess_name(self, name: str, source_language: str) -> Dict[str, str]:
        """Generate multiple normalized representations of a character name"""
        if not name:
            return self._empty_representations()

        # Basic normalization
        normalized = self._normalize_unicode(name.strip())

        # Generate representations based on language
        if source_language == "japanese":
            return self._process_japanese_name(normalized)
        else:
            return self._process_english_name(normalized)

    def _normalize_unicode(self, text: str) -> str:
        """Normalize Unicode characters"""
        # NFD normalization + case folding for consistent comparison
        return unicodedata.normalize("NFD", text).casefold()

    def _process_japanese_name(self, name: str) -> Dict[str, str]:
        """Process Japanese character names with multiple script representations"""
        representations = {
            "original": name,
            "normalized": self._normalize_unicode(name),
            "hiragana": jaconv.kata2hira(name) if self._is_katakana(name) else name,
            "katakana": jaconv.hira2kata(name) if self._is_hiragana(name) else name,
            "romaji": self._to_romaji(name),
            "phonetic": self._get_phonetic_key(self._to_romaji(name)),
            "tokens": self._tokenize_name(name),
        }
        return representations

    def _is_katakana(self, text: str) -> bool:
        """Check if text contains katakana characters"""
        return any("KATAKANA" in unicodedata.name(c, "") for c in text)

    def _is_hiragana(self, text: str) -> bool:
        """Check if text contains hiragana characters"""
        return any("HIRAGANA" in unicodedata.name(c, "") for c in text)

    def _process_english_name(self, name: str) -> Dict[str, str]:
        """Process English character names with intelligent normalization"""
        # Handle common anime character name patterns
        normalized_name = name

        # Normalize common punctuation and formatting
        normalized_name = normalized_name.replace(".", "").replace(",", "")
        normalized_name = normalized_name.replace("  ", " ").strip()

        # Handle name order variations (e.g., "Monkey D., Luffy" vs "Luffy Monkey")
        tokens = normalized_name.split()
        if len(tokens) >= 2:
            # Create variations: original order and reversed
            original_order = " ".join(tokens)
            if len(tokens) >= 3:
                # For 3+ tokens, try different combinations
                first_last = f"{tokens[0]} {tokens[-1]}"
                last_first = f"{tokens[-1]} {tokens[0]}"
            else:
                first_last = original_order
                last_first = " ".join(reversed(tokens))
        else:
            first_last = last_first = normalized_name

        representations = {
            "original": name,
            "normalized": self._normalize_unicode(normalized_name),
            "first_last": first_last,
            "last_first": last_first,
            "phonetic": self._get_phonetic_key(normalized_name),
            "soundex": jellyfish.soundex(normalized_name),
            "metaphone": jellyfish.metaphone(normalized_name),
            "tokens": self._tokenize_name(normalized_name),
        }
        return representations

    def _to_romaji(self, japanese_text: str) -> str:
        """Convert Japanese text to romaji"""
        try:
            result = self.conv.do(japanese_text)
            return str(result)
        except:
            return japanese_text

    def _get_phonetic_key(self, text: str) -> str:
        """Generate phonetic representation"""
        if text.isascii():
            result = jellyfish.metaphone(text)
            return str(result) if result else text
        return text

    def _tokenize_name(self, name: str) -> str:
        """Break name into tokens"""
        # Split on common delimiters
        tokens = re.split(r"[\s\-_.,()]+", name)
        return " ".join([t.strip() for t in tokens if t.strip()])

    def _empty_representations(self) -> Dict[str, str]:
        """Return empty representations for null names"""
        return {"original": "", "normalized": "", "phonetic": "", "tokens": ""}

    @staticmethod
    def add_name_variation(
        variations_dict: Dict[str, str], name: Optional[str]
    ) -> Dict[str, str]:
        """
        Add name to variations dict with case-insensitive deduplication.
        For CJK characters, also normalizes spacing to prevent duplicates like "ç¶¾ç€¬ æ¡ƒ" vs "ç¶¾ç€¬æ¡ƒ".

        Uses lowercase key to preserve first occurrence and prevent case duplicates.
        E.g., if "Naruto" exists, "naruto" won't be added.
        For CJK: if "ç¶¾ç€¬ æ¡ƒ" exists, "ç¶¾ç€¬æ¡ƒ" won't be added.

        Args:
            variations_dict: Existing dict mapping normalized names to original names
            name: Name to add (original casing)

        Returns:
            Updated variations dict
        """
        if name:
            # Check if name contains CJK characters
            has_cjk = any(
                "\u4e00" <= c <= "\u9fff"  # Chinese/Japanese kanji
                or "\u3040" <= c <= "\u309f"  # Hiragana
                or "\u30a0" <= c <= "\u30ff"  # Katakana
                or "\uac00" <= c <= "\ud7af"  # Korean
                for c in name
            )

            if has_cjk:
                # For CJK: normalize spacing and lowercase for comparison
                normalized_key = name.replace(" ", "").lower()
            else:
                # For non-CJK: only lowercase for comparison (existing behavior)
                normalized_key = name.lower()

            # Only add if not already present (preserve first occurrence)
            if normalized_key not in variations_dict:
                variations_dict[normalized_key] = name
        return variations_dict

    @staticmethod
    def fill_missing_field(
        current_value: Optional[Any],
        source_value: Optional[Any],
        convert_to_str: bool = False,
    ) -> Optional[Any]:
        """
        Fill missing field with fallback priority logic.

        Only fills if current value is falsy (None, "", 0, False, etc.)
        AND source has a truthy value. Preserves first occurrence.

        Args:
            current_value: Current field value
            source_value: New value from source
            convert_to_str: If True, wrap result with str()

        Returns:
            Updated value (current if truthy, source if current falsy)

        Examples:
            >>> fill_missing_field(None, "value")
            "value"
            >>> fill_missing_field("existing", "new")
            "existing"
            >>> fill_missing_field(None, 25, convert_to_str=True)
            "25"
        """
        # Sanitize placeholder values from AnimePlanet to None
        if source_value == "?":
            source_value = None

        if not current_value and source_value:
            return str(source_value) if convert_to_str else source_value
        return current_value

    @staticmethod
    def deduplicate_nicknames(nicknames: List[str], new_nickname: str) -> List[str]:
        """
        Smart nickname deduplication that handles exact matches and parentheses patterns.

        Handles:
        - Exact duplicates (case-insensitive)
        - Subset duplicates: "Okarun" vs "Okarun (ã‚ªã‚«ãƒ«ãƒ³)" - keeps only the combined one

        Args:
            nicknames: Existing list of nicknames
            new_nickname: New nickname to add

        Returns:
            Updated list of nicknames
        """
        if not new_nickname:
            return nicknames

        new_lower = new_nickname.lower()
        should_add = True
        nicknames_to_remove = []

        # Check against existing nicknames
        for i, existing in enumerate(nicknames):
            existing_lower = existing.lower()

            # Exact match - skip
            if new_lower == existing_lower:
                should_add = False
                break

            # New nickname is more complete (has parentheses, existing doesn't)
            # e.g., existing="Okarun", new="Okarun (ã‚ªã‚«ãƒ«ãƒ³)"
            if new_lower.startswith(existing_lower + " (") and ")" in new_nickname:
                # Mark existing for removal, will add new one
                nicknames_to_remove.append(i)
                should_add = True

            # Existing nickname is more complete (has parentheses, new doesn't)
            # e.g., existing="Okarun (ã‚ªã‚«ãƒ«ãƒ³)", new="Okarun"
            elif existing_lower.startswith(new_lower + " (") and ")" in existing:
                # Keep existing, skip new
                should_add = False
                break

        # Remove less complete versions (reverse order to maintain indices)
        for idx in sorted(nicknames_to_remove, reverse=True):
            nicknames.pop(idx)

        # Add new nickname if appropriate
        if should_add:
            nicknames.append(new_nickname)

        return nicknames


class EnsembleFuzzyMatcher:
    """Multi-algorithm fuzzy matching with ensemble scoring (enhanced)"""

    def __init__(self, model_name: str = "BAAI/bge-m3", enable_visual: bool = True):
        """
        Initialize the ensemble fuzzy matcher with optional multilingual embedding and vision processors.
        
        Parameters:
            model_name (str): Name or path of the text embedding model to load.
            enable_visual (bool): If True, attempts to initialize a vision processor for image-based similarity.
        
        Notes:
            - If the embedding model fails to load, `self.embedding_model` will be set to `None` and text-only fallback matching is used.
            - Visual matching is enabled only when `enable_visual` is True and the vision components are available; on failure `self.enable_visual` is set to False and `self.vision_processor` remains `None`.
        """
        try:
            self.embedding_model = SentenceTransformer(
                model_name,
            )
            logger.info(f"Loaded embedding model: {model_name}")
        except Exception as e:
            logger.warning(
                f"Failed to load BGE-M3 {model_name}, falling back to basic matching: {e}"
            )
            self.embedding_model = None  # type: ignore[assignment]

        # Initialize vision model for character image similarity
        self.enable_visual = enable_visual and VISION_AVAILABLE
        self.vision_processor: Optional[VisionProcessor] = None

        if self.enable_visual:
            try:
                if Settings is not None and VisionProcessor is not None:
                    settings = Settings()
                    self.vision_processor = VisionProcessor(settings)
                    self.ccips = LegacyCCIPS()
                    logger.info(
                        f"Visual character matching enabled with CCIP (fallback: {settings.image_embedding_model})"
                    )
            except Exception as e:
                logger.warning(
                    f"Failed to initialize vision processor, visual matching disabled: {e}"
                )
                self.enable_visual = False

    async def calculate_visual_similarity(
        self, image_url1: Optional[str], image_url2: Optional[str]
    ) -> float:
        """Calculate visual similarity between two character images using CCIP

        Uses DeepGHS CCIP model specialized for anime character recognition.
        Falls back to OpenCLIP if CCIP is unavailable.

        Args:
            image_url1: URL of first character image
            image_url2: URL of second character image

        Returns:
            Visual similarity score [0.0, 1.0], or 0.0 if calculation fails
        """
        if not self.enable_visual or not self.vision_processor:
            return 0.0

        if not image_url1 or not image_url2:
            return 0.0

        try:
            # Use CCIP from LegacyCCIPS
            similarity = self.ccips.calculate_character_similarity(
                image_url1, image_url2
            )

            # Ensure result is in [0.0, 1.0] range
            similarity = max(0.0, min(1.0, similarity))

            return similarity

        except Exception as e:
            logger.debug(f"Visual similarity calculation failed: {e}")
            return 0.0

    async def calculate_similarity(
        self,
        name1_repr: Dict[str, str],
        name2_repr: Dict[str, str],
        candidate_aliases: Optional[List[str]] = None,
        source: str = "generic",
        jikan_image_url: Optional[str] = None,
        candidate_image_url: Optional[str] = None,
    ) -> float:
        """
        Calculate ensemble similarity score between two character name representations
        Supports alias expansion, order-insensitive matching, adaptive weights, and visual verification

        Args:
            name1_repr: Primary character name representations
            name2_repr: Candidate character name representations
            candidate_aliases: Additional name aliases for candidate
            source: Source type (anilist, anidb, animeplanet, etc.)
            jikan_image_url: Image URL from Jikan/MAL for visual verification
            candidate_image_url: Image URL from candidate source for visual verification

        Returns:
            Ensemble similarity score [0.0, 1.0]
        """

        # Expand aliases: compare primary name against all aliases + candidate name
        candidate_names = [name2_repr.get("original", "")]
        if candidate_aliases:
            candidate_names += candidate_aliases

        best_score = 0.0

        for candidate_name in candidate_names:
            candidate_repr = name2_repr.copy()
            candidate_repr["original"] = candidate_name
            candidate_repr["tokens"] = self._tokenize_name(candidate_name)
            candidate_repr["phonetic"] = self._get_phonetic_key(candidate_name)
            candidate_repr["first_last"] = candidate_name
            candidate_repr["last_first"] = " ".join(candidate_name.split()[::-1])

            # Calculate weighted ensemble score
            scores = {}

            # 1. Enhanced semantic similarity (tests all name variations + Japanese)
            scores["semantic"] = self._enhanced_semantic_similarity(
                name1_repr, candidate_repr
            )

            # 2. Edit distance similarity
            scores["edit_distance"] = self._edit_distance_similarity(
                name1_repr.get("normalized", ""), candidate_repr.get("normalized", "")
            )

            # 3. Token-based similarity (order-insensitive fallback)
            scores["token_based"] = self._token_similarity(
                name1_repr.get("tokens", ""), candidate_repr.get("tokens", "")
            )
            # Fallback: token set ratio
            scores["token_set"] = self._token_set_similarity(
                name1_repr.get("tokens", ""), candidate_repr.get("tokens", "")
            )

            # 4. Phonetic similarity
            scores["phonetic"] = self._phonetic_similarity(
                name1_repr.get("phonetic", ""), candidate_repr.get("phonetic", "")
            )

            # 5. Name order variations
            scores["name_order"] = max(
                self._edit_distance_similarity(
                    name1_repr.get("first_last", ""),
                    candidate_repr.get("first_last", ""),
                ),
                self._edit_distance_similarity(
                    name1_repr.get("first_last", ""),
                    candidate_repr.get("last_first", ""),
                ),
                self._edit_distance_similarity(
                    name1_repr.get("last_first", ""),
                    candidate_repr.get("first_last", ""),
                ),
            )

            # 6. Visual similarity (character image verification)
            # Calculate once per best candidate to avoid redundant downloads
            if jikan_image_url and candidate_image_url:
                # Run visual similarity asynchronously
                visual_score = await self.calculate_visual_similarity(
                    jikan_image_url, candidate_image_url
                )
                scores["visual"] = visual_score
            else:
                scores["visual"] = 0.0

            # Adaptive weights - OPTIMIZED: Source-specific tuning with visual verification
            if source.lower() == "anilist":
                weights = {
                    "semantic": 0.45,  # Reduced from 0.6 to make room for visual
                    "visual": 0.30,  # NEW - strong visual verification
                    "edit_distance": 0.05,
                    "token_based": 0.15,  # Reduced from 0.25
                    "token_set": 0.03,  # Reduced from 0.05
                    "phonetic": 0.02,  # Reduced from 0.05
                    "name_order": 0.0,  # Often fails due to different name ordering conventions
                }
            elif source.lower() == "anidb":
                # AniDB-SPECIFIC OPTIMIZATION: Enhanced weights for standardized AniDB format
                # Heavily prioritize semantic, with visual verification layer
                weights = {
                    "semantic": 0.65,  # Reduced from 0.95 to make room for visual
                    "visual": 0.25,  # NEW - visual verification layer
                    "edit_distance": 0.02,  # SAFETY NET: Catch typos/misspellings
                    "token_based": 0.08,  # Increased from 0.03 for validation
                    "token_set": 0.0,  # DISABLED: Not useful for partial name matching
                    "phonetic": 0.0,  # DISABLED: Not useful for partial name matching
                    "name_order": 0.0,  # DISABLED: Handled by enhanced semantic similarity
                }
            elif source.lower() == "animeplanet":
                # AnimePlanet-SPECIFIC OPTIMIZATION: Similar to AniList but with stronger semantic
                # AnimePlanet uses uppercase surnames (e.g., "Ken TAKAKURA") which needs normalization
                weights = {
                    "semantic": 0.50,  # Reduced from 0.75 to make room for visual
                    "visual": 0.30,  # NEW - strong visual verification
                    "edit_distance": 0.05,  # SAFETY NET: Handle case differences
                    "token_based": 0.10,  # Reduced from 0.15
                    "token_set": 0.05,  # BACKUP: Set-based validation
                    "phonetic": 0.0,  # DISABLED: Not useful for English names
                    "name_order": 0.0,  # DISABLED: Handled by semantic similarity
                }
            else:
                # Default weights for other sources
                weights = {
                    "semantic": 0.50,  # Reduced from 0.7 to make room for visual
                    "visual": 0.25,  # NEW - visual verification
                    "edit_distance": 0.05,  # Reduced: Different name orders cause issues
                    "token_based": 0.12,  # Reduced from 0.15
                    "token_set": 0.05,  # Use as secondary validation
                    "phonetic": 0.03,  # Reduced from 0.05
                    "name_order": 0.0,  # DISABLED: Unreliable for anime character names
                }

            ensemble_score = sum(scores.get(k, 0.0) * w for k, w in weights.items())
            best_score = max(best_score, ensemble_score)

            # Get semantic and visual scores for logging
            semantic_score = scores.get("semantic", 0.0)
            visual_score = scores.get("visual", 0.0)

            # Log match details when scores are significant
            if ensemble_score >= 0.8:
                logger.info(
                    f"Match '{name1_repr.get('original', '')}' vs '{name2_repr.get('original', '')}' ({source.upper()}): "
                    f"text={semantic_score:.3f}, visual={visual_score:.3f}, ensemble={ensemble_score:.3f}"
                )

        return best_score

    def _standardize_for_embedding(self, text: str) -> str:
        """Aggressive normalization for embedding consistency across sources.

        Removes format variations that create different embeddings for same character:
        - Punctuation differences (D. vs D)
        - Case sensitivity (Luffy vs luffy)
        - Extra whitespace
        """
        if not text:
            return ""

        # Remove punctuation except hyphens (preserve compound names like "Roronoa-Zoro")
        text = re.sub(r'[.,;:!?"\']', "", text)

        # Normalize whitespace
        text = " ".join(text.split())

        # Lowercase for case-insensitive matching (BGE-M3 is case-sensitive!)
        text = text.lower()

        return text.strip()

    def _semantic_similarity(self, text1: str, text2: str) -> float:
        if not self.embedding_model or not text1 or not text2:
            return 0.0
        try:
            # Pre-normalize both texts for consistent embeddings
            norm1 = self._standardize_for_embedding(text1)
            norm2 = self._standardize_for_embedding(text2)

            embeddings = self.embedding_model.encode([norm1, norm2])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return float(max(0.0, similarity))
        except Exception as e:
            logger.warning(f"Semantic similarity failed: {e}")
            return 0.0

    def _enhanced_semantic_similarity(
        self, name1_repr: Dict[str, str], name2_repr: Dict[str, str]
    ) -> float:
        """Enhanced semantic similarity testing all name variations and Japanese text.

        This function addresses the critical issue where character names are reversed
        between Jikan/MAL and AniDB sources (e.g., "Robin Nico" vs "Nico Robin").
        BGE-M3's multilingual capabilities are leveraged for Japanese text matching.
        """
        if not self.embedding_model:
            return 0.0

        # Extract all name variations for both characters
        name1_variants = []
        name2_variants = []

        # Add standard name variations with AniDB optimization
        for name_repr, variants in [
            (name1_repr, name1_variants),
            (name2_repr, name2_variants),
        ]:
            # Original name (may include Japanese with "|" separator)
            original = name_repr.get("original", "").strip()
            if original:
                variants.append(original)

                # Split Japanese/native names if present (format: "English Name | Japanese Name")
                if "|" in original:
                    parts = [p.strip() for p in original.split("|")]
                    variants.extend([p for p in parts if p])

                    # AniDB ENHANCEMENT: Add clean English-only version for better matching
                    english_part = parts[0].strip() if parts else ""
                    if english_part and english_part not in variants:
                        variants.append(english_part)

            # Add name order variations
            first_last = name_repr.get("first_last", "").strip()
            if first_last and first_last not in variants:
                variants.append(first_last)

            last_first = name_repr.get("last_first", "").strip()
            if last_first and last_first not in variants:
                variants.append(last_first)

            # Add normalized version
            normalized = name_repr.get("normalized", "").strip()
            if normalized and normalized not in variants:
                variants.append(normalized)

            # AniDB ENHANCEMENT: Add middle name variations for characters like "Monkey D. Luffy"
            if original and any(
                token in original.lower() for token in ["d.", "d ", " d "]
            ):
                # Handle middle initial patterns common in One Piece characters
                middle_variants = []
                if "d." in original.lower():
                    # "Monkey D. Luffy" -> "Monkey Luffy"
                    no_middle = original.replace("D.", "").replace("d.", "").strip()
                    no_middle = " ".join(no_middle.split())  # Clean extra spaces
                    if no_middle and no_middle not in variants:
                        middle_variants.append(no_middle)

                # Add these optimized variants
                variants.extend(middle_variants)

        # Test ALL combinations and return the BEST semantic score
        max_similarity = 0.0
        best_pair = None
        all_comparisons = []  # Track all comparisons for debugging

        for n1 in name1_variants:
            for n2 in name2_variants:
                if n1 and n2:  # Ensure both names are non-empty
                    similarity = self._semantic_similarity(n1, n2)
                    all_comparisons.append((n1, n2, similarity))
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_pair = (n1, n2)

        # Log best semantic match only
        if best_pair and max_similarity >= 0.85:
            logger.info(
                f"SEMANTIC MATCH: '{best_pair[0]}' <-> '{best_pair[1]}' = {max_similarity:.3f}"
            )

        return max_similarity

    def _edit_distance_similarity(self, text1: str, text2: str) -> float:
        if not text1 or not text2:
            return 0.0
        return fuzz.ratio(text1, text2) / 100.0

    def _phonetic_similarity(self, phonetic1: str, phonetic2: str) -> float:
        if not phonetic1 or not phonetic2:
            return 0.0
        if phonetic1 == phonetic2:
            return 1.0
        return fuzz.ratio(phonetic1, phonetic2) / 100.0

    def _token_similarity(self, tokens1: str, tokens2: str) -> float:
        if not tokens1 or not tokens2:
            return 0.0
        return fuzz.token_sort_ratio(tokens1, tokens2) / 100.0

    def _token_set_similarity(self, tokens1: str, tokens2: str) -> float:
        if not tokens1 or not tokens2:
            return 0.0
        return fuzz.token_set_ratio(tokens1, tokens2) / 100.0

    def _tokenize_name(self, name: str) -> str:
        tokens = re.split(r"[\s\-_.,()]+", name)
        return " ".join([t.strip() for t in tokens if t.strip()])

    def _get_phonetic_key(self, text: str) -> str:
        if text.isascii():
            result = jellyfish.metaphone(text)
            return str(result) if result else text
        return text


class MatchValidationClassifier:
    """LLM-powered validation for character matches"""

    def __init__(self, llm_client: Any = None) -> None:
        self.llm_client = llm_client

    async def validate_match(
        self, char1: Dict[str, Any], char2: Dict[str, Any], similarity_score: float
    ) -> Tuple[bool, str]:
        """Use LLM to validate character matches with context"""

        # For now, use rule-based validation
        # TODO: Integrate with actual LLM client when available
        return self._rule_based_validation(char1, char2, similarity_score)

    def _rule_based_validation(
        self, char1: Dict[str, Any], char2: Dict[str, Any], similarity_score: float
    ) -> Tuple[bool, str]:
        """Rule-based validation fallback"""

        validation_notes = []

        # High confidence threshold
        if similarity_score > 0.9:
            return True, "High similarity score with strong matching evidence"

        # Check role consistency
        role1 = char1.get("role", "").lower()
        role2 = char2.get("role", "").lower()

        if role1 and role2:
            role_mapping = {
                "main": ["main", "primary", "protagonist"],
                "supporting": ["supporting", "secondary"],
                "background": ["background", "minor"],
            }

            roles_match = any(
                role1 in roles and role2 in roles for roles in role_mapping.values()
            )

            if not roles_match:
                validation_notes.append("Role mismatch detected")

        # Check voice actor consistency (if available)
        vas1 = char1.get("voice_actors", [])
        vas2 = char2.get("voice_actors", [])

        if vas1 and vas2:
            va_names1 = set()
            va_names2 = set()

            # Handle different voice actor formats
            for va in vas1:
                if isinstance(va, dict):
                    name = va.get("name", "")
                    if isinstance(name, str):
                        va_names1.add(name.lower())
                elif isinstance(va, str):
                    va_names1.add(va.lower())

            for va in vas2:
                if isinstance(va, dict):
                    name = va.get("name", "")
                    if isinstance(name, str):
                        va_names2.add(name.lower())
                elif isinstance(va, str):
                    va_names2.add(va.lower())

            if va_names1 & va_names2:  # Common voice actors
                validation_notes.append("Voice actor match confirms identity")
                similarity_score += 0.1  # Boost confidence

        # Medium confidence validation
        if similarity_score > 0.5:
            notes = (
                "; ".join(validation_notes)
                if validation_notes
                else "Medium confidence match"
            )
            return True, notes

        # Low confidence but still accept for testing
        if similarity_score > 0.3:
            notes = (
                "; ".join(validation_notes)
                if validation_notes
                else "Low confidence match"
            )
            return True, notes

        return False, f"Very low confidence: {similarity_score:.3f}"


class AICharacterMatcher:
    """Main AI-powered character matching system"""

    def __init__(self) -> None:
        self.language_detector = LanguageDetector()
        self.preprocessor = CharacterNamePreprocessor()
        self.fuzzy_matcher = EnsembleFuzzyMatcher()
        self.validator = MatchValidationClassifier()

        logger.info("AI Character Matcher initialized")

    async def match_characters(
        self,
        jikan_chars: List[Dict[str, Any]],
        anilist_chars: List[Dict[str, Any]],
        anidb_chars: List[Dict[str, Any]],
        anime_planet_chars: Optional[List[Dict[str, Any]]] = None,
    ) -> List[ProcessedCharacter]:
        """Main entry point for character matching across all sources"""

        # Handle optional anime_planet_chars
        if anime_planet_chars is None:
            anime_planet_chars = []

        logger.info(
            f"Starting character matching: Jikan={len(jikan_chars)}, AniList={len(anilist_chars)}, "
            f"AniDB={len(anidb_chars)}, AnimePlanet={len(anime_planet_chars)}"
        )

        # Use Jikan as primary source (most comprehensive)
        processed_characters = []

        for jikan_char in jikan_chars:
            char_name = self._extract_primary_name(jikan_char, "jikan") or "Unknown"

            # Find matches in other sources
            anilist_match = None
            anidb_match = None
            anime_planet_match = None

            # Test AniList first
            anilist_match = await self._find_best_match(
                jikan_char, anilist_chars, "anilist"
            )

            # Always search AniDB (no cross-source termination)
            anidb_match = await self._find_best_match(jikan_char, anidb_chars, "anidb")

            # Search AnimePlanet if available
            if anime_planet_chars:
                anime_planet_match = await self._find_best_match(
                    jikan_char, anime_planet_chars, "animeplanet"
                )

            # Log if high-confidence matches were found
            anilist_score = anilist_match.similarity_score if anilist_match else 0.0
            anidb_score = anidb_match.similarity_score if anidb_match else 0.0
            anime_planet_score = (
                anime_planet_match.similarity_score if anime_planet_match else 0.0
            )

            # Consolidate match confidence logging into one line
            high_conf_matches = []
            med_conf_matches = []

            if anilist_score >= 0.9:
                high_conf_matches.append(f"ANILIST: {anilist_score:.3f}")
            elif anilist_score >= 0.7:
                med_conf_matches.append(f"ANILIST: {anilist_score:.3f}")

            if anidb_score >= 0.9:
                high_conf_matches.append(f"ANIDB: {anidb_score:.3f}")
            elif anidb_score >= 0.7:
                med_conf_matches.append(f"ANIDB: {anidb_score:.3f}")

            if anime_planet_score >= 0.9:
                high_conf_matches.append(f"ANIMEPLANET: {anime_planet_score:.3f}")
            elif anime_planet_score >= 0.7:
                med_conf_matches.append(f"ANIMEPLANET: {anime_planet_score:.3f}")

            if high_conf_matches:
                logger.info(f"HIGH-CONFIDENCE MATCHES: {' | '.join(high_conf_matches)}")
            if med_conf_matches:
                logger.info(
                    f"MEDIUM-CONFIDENCE MATCHES: {' | '.join(med_conf_matches)} for '{char_name}'"
                )

            # Safety monitoring: Log when no match found (could indicate missing data)
            if anilist_score == 0.0:
                logger.debug(f"No AniList match found for '{char_name}'")
            if anidb_score == 0.0:
                logger.debug(f"No AniDB match found for '{char_name}'")
            if anime_planet_score == 0.0:
                logger.debug(f"No AnimePlanet match found for '{char_name}'")

            # Integrate data from all matched sources
            integrated_char = await self._integrate_character_data(
                jikan_char, anilist_match, anidb_match, anime_planet_match
            )

            # Store match scores for stage5 to use (will be removed before final output)
            integrated_char.match_scores = {
                "anilist": anilist_score,
                "anidb": anidb_score,
                "animeplanet": anime_planet_score,
            }

            processed_characters.append(integrated_char)

        return processed_characters

    async def _find_best_match(
        self,
        primary_char: Dict[str, Any],
        candidate_chars: List[Dict[str, Any]],
        source: str,
    ) -> Optional[CharacterMatch]:
        """Find the best matching character from a source with visual verification"""

        if not candidate_chars:
            return None

        primary_name = self._extract_primary_name(
            primary_char, "jikan", target_source=source
        )
        if not primary_name:
            return None

        # Detect language and preprocess primary character name
        language = self.language_detector.detect_language(primary_name)
        primary_repr = self.preprocessor.preprocess_name(primary_name, language)

        # Extract Jikan image URL for visual verification
        jikan_image_url = self._extract_image_url(primary_char, "jikan")

        best_match = None
        best_score = 0.0

        for candidate_char in candidate_chars:
            candidate_name = self._extract_primary_name(candidate_char, source)
            if not candidate_name:
                continue

            # Extract candidate ID for logging
            candidate_id = (
                candidate_char.get("mal_id")
                or candidate_char.get("id")
                or candidate_char.get("anilist_id")
                or candidate_char.get("anidb_id")
                or "N/A"
            )

            # Preprocess candidate name
            candidate_language = self.language_detector.detect_language(candidate_name)
            candidate_repr = self.preprocessor.preprocess_name(
                candidate_name, candidate_language
            )

            # Extract candidate image URL for visual verification
            candidate_image_url = self._extract_image_url(candidate_char, source)

            # Calculate similarity with visual verification
            similarity_score = await self.fuzzy_matcher.calculate_similarity(
                primary_repr,
                candidate_repr,
                source=source,
                jikan_image_url=jikan_image_url,
                candidate_image_url=candidate_image_url,
            )

            if similarity_score > best_score:
                best_score = similarity_score
                best_match = candidate_char

            # Early termination: If we find a very high confidence match, return immediately
            if similarity_score >= 0.9:
                # Extract character IDs and names from both characters
                primary_id = (
                    primary_char.get("mal_id") or primary_char.get("id") or "N/A"
                )
                candidate_id = (
                    candidate_char.get("mal_id")
                    or candidate_char.get("id")
                    or candidate_char.get("anilist_id")
                    or candidate_char.get("anidb_id")
                    or "N/A"
                )

                # Use the source parameter passed to this function
                source_name = source.upper()

                logger.info(
                    f"EARLY TERMINATION: {similarity_score:.6f} - '{primary_name}' â†’ '{candidate_name}' ({source_name})"
                )

                # Validate the high-confidence match immediately
                is_valid, notes = await self.validator.validate_match(
                    primary_char, candidate_char, similarity_score
                )
                if is_valid:
                    confidence = self._determine_confidence(similarity_score)
                    return CharacterMatch(
                        source_char=primary_char,
                        target_char=candidate_char,
                        confidence=confidence,
                        similarity_score=similarity_score,
                        matching_evidence={"ensemble": similarity_score},
                        validation_notes=notes,
                    )
                # If validation fails, continue searching
                best_score = similarity_score
                best_match = candidate_char

        # Only accept matches with score >= 0.7 (MEDIUM confidence minimum)
        # Anything below indicates system needs improvement (e.g., name variations like "Sanji" vs "Vinsmoke Sanji")
        if best_match and best_score >= 0.7:
            # Validate the match
            is_valid, notes = await self.validator.validate_match(
                primary_char, best_match, best_score
            )

            if is_valid:
                confidence = self._determine_confidence(best_score)

                return CharacterMatch(
                    source_char=primary_char,
                    target_char=best_match,
                    confidence=confidence,
                    similarity_score=best_score,
                    matching_evidence={"ensemble_score": best_score},
                    validation_notes=notes,
                )
        else:
            # Safety monitoring: Log rejected matches near threshold (0.6-0.69)
            if best_match and 0.6 <= best_score < 0.7:
                candidate_name = (
                    self._extract_primary_name(best_match, source) or "Unknown"
                )
                logger.warning(
                    f"ðŸš« REJECTED (near threshold): '{primary_name}' â†’ '{candidate_name}' "
                    f"({source.upper()}) score={best_score:.6f} (need â‰¥0.7)"
                )

        return None

    def _extract_primary_name(
        self,
        character: Dict[str, Any],
        source: str,
        target_source: Optional[str] = None,
    ) -> Optional[str]:
        """Extract the primary name from a character based on source format

        Args:
            character: Character data dictionary
            source: Source of this character data (jikan, anilist, anidb)
            target_source: Target source we're matching against (used for Jikan to decide if Japanese should be included)
        """

        if source == "jikan":
            # Jikan format: character.name or name (depending on processing stage)
            if "character" in character and "name" in character["character"]:
                primary_name = str(character["character"]["name"])
                name_native = character["character"].get("name_kanji")
            else:
                # Direct character object (from detailed characters)
                primary_name = str(character.get("name", ""))
                name_native = character.get("name_kanji")

            # IMPORTANT: Only include Japanese/native name when matching against AniList
            # AniDB uses English-only names, so including Japanese hurts similarity scores
            if name_native and target_source == "anilist":
                primary_name += f" | {name_native}"

            return primary_name
        elif source == "anilist":
            name_obj = character.get("name", {})
            primary_name = str(name_obj.get("full") or name_obj.get("first", ""))
            # Include native/Japanese name for better cross-matching
            native_name = name_obj.get("native")
            if native_name:
                primary_name += f" | {native_name}"
            return primary_name
        elif source == "anidb":
            return str(character.get("name", ""))
        elif source == "animeplanet":
            # AnimePlanet format: direct 'name' field with uppercase surnames (e.g., "Ken TAKAKURA")
            return str(character.get("name", ""))

        return None

    def _extract_image_url(
        self, character: Dict[str, Any], source: str
    ) -> Optional[str]:
        """Extract primary image URL from character data based on source format

        Args:
            character: Character data dictionary
            source: Source of this character data (jikan, anilist, anidb, animeplanet)

        Returns:
            Image URL or None if not available
        """
        try:
            if source == "jikan":
                # Try detailed format first (images.jpg.image_url)
                if "images" in character and isinstance(character["images"], dict):
                    jpg_images = character["images"].get("jpg", {})
                    if isinstance(jpg_images, dict):
                        image_url = jpg_images.get("image_url")
                        if image_url:
                            return str(image_url)

                # Try nested character object format
                if "character" in character and "images" in character["character"]:
                    char_images = character["character"]["images"]
                    if isinstance(char_images, dict):
                        jpg_images = char_images.get("jpg", {})
                        if isinstance(jpg_images, dict):
                            image_url = jpg_images.get("image_url")
                            if image_url:
                                return str(image_url)

            elif source == "anilist":
                # AniList format: image.large
                if "image" in character and isinstance(character["image"], dict):
                    image_url = character["image"].get("large")
                    if image_url:
                        return str(image_url)

            elif source == "anidb":
                # AniDB format: construct URL from 'picture' field
                picture = character.get("picture")
                if picture:
                    return f"https://cdn.anidb.net/images/main/{picture}"

            elif source == "animeplanet":
                # AnimePlanet format: direct 'image' field
                image_url = character.get("image")
                if image_url:
                    return str(image_url)

        except Exception as e:
            logger.debug(f"Failed to extract image URL from {source}: {e}")

        return None

    def _determine_confidence(self, score: float) -> MatchConfidence:
        """Determine confidence level based on similarity score"""
        if score >= 0.7:
            return MatchConfidence.HIGH
        else:
            return MatchConfidence.LOW

    async def _integrate_character_data(
        self,
        jikan_char: Dict[str, Any],
        anilist_match: Optional[CharacterMatch],
        anidb_match: Optional[CharacterMatch],
        anime_planet_match: Optional[CharacterMatch] = None,
    ) -> ProcessedCharacter:
        """Integrate character data from multiple sources with hierarchical priority"""

        # Start with Jikan as base (most comprehensive)
        # Handle Jikan data format (character.name vs name)
        jikan_name = ""
        jikan_url = ""

        if "character" in jikan_char:
            # Raw Jikan API format
            char_data = jikan_char["character"]
            jikan_name = char_data.get("name", "")
            char_data.get("mal_id")
            jikan_url = char_data.get("url", "")
        else:
            # Processed format (from characters_detailed.json)
            jikan_name = jikan_char.get("name", "")
            # Jikan uses 'character_id' field, not 'mal_id'
            jikan_char.get("character_id") or jikan_char.get("mal_id")
            jikan_url = jikan_char.get("url", "")

        logger.debug(
            f"Integrating Jikan character: {jikan_name} (role: {jikan_char.get('role', 'Unknown')})"
        )

        integrated = ProcessedCharacter(
            name=jikan_name,
            role=jikan_char.get("role", ""),
            name_variations=[],
            name_native=jikan_char.get(
                "name_kanji"
            ),  # From Jikan, fallback to AniList later
            nicknames=jikan_char.get("nicknames", []),
            voice_actors=self._extract_voice_actors(jikan_char),
            character_pages={"mal": jikan_url},
            images=[],
            age=None,
            description=jikan_char.get("about"),
            gender=None,
            hair_color=None,
            eye_color=None,
            character_traits=[],
            favorites=jikan_char.get("favorites"),  # MAL favorites count
            match_confidence=MatchConfidence.HIGH,  # Primary source
            source_count=1,
        )

        # Collect all name variations with case-insensitive deduplication
        # Use dict to preserve first occurrence while ignoring case duplicates
        name_variations_dict: Dict[str, str] = {}
        name_variations_dict = CharacterNamePreprocessor.add_name_variation(
            name_variations_dict, integrated.name
        )
        name_variations_dict = CharacterNamePreprocessor.add_name_variation(
            name_variations_dict, integrated.name_native
        )

        # Collect all images
        images = []
        # Handle Jikan image format
        if "character" in jikan_char and "images" in jikan_char["character"]:
            char_images = jikan_char["character"]["images"]
            if "jpg" in char_images and "image_url" in char_images["jpg"]:
                images.append(char_images["jpg"]["image_url"])
        elif jikan_char.get("images", {}).get("jpg", {}).get("image_url"):
            images.append(jikan_char["images"]["jpg"]["image_url"])

        # Integrate AniList data - ADD name variations and missing data
        if anilist_match:
            anilist_char = anilist_match.target_char
            integrated.source_count += 1

            # Add AniList name variations
            anilist_name = anilist_char.get("name", {})
            name_variations_dict = CharacterNamePreprocessor.add_name_variation(
                name_variations_dict, anilist_name.get("full")
            )
            if anilist_name.get("native"):
                # Set native name with fallback: use AniList if Jikan didn't have it
                if not integrated.name_native:
                    integrated.name_native = anilist_name["native"]
                name_variations_dict = CharacterNamePreprocessor.add_name_variation(
                    name_variations_dict, anilist_name["native"]
                )

            # Merge alternative and alternativeSpoiler into nicknames with smart deduplication
            alternatives = anilist_name.get("alternative", []) or []
            spoilers = anilist_name.get("alternativeSpoiler", []) or []

            for alt_name in alternatives + spoilers:
                integrated.nicknames = CharacterNamePreprocessor.deduplicate_nicknames(
                    integrated.nicknames, alt_name
                )

            # Fill missing data from AniList
            integrated.age = CharacterNamePreprocessor.fill_missing_field(
                integrated.age, anilist_char.get("age"), convert_to_str=True
            )
            integrated.gender = CharacterNamePreprocessor.fill_missing_field(
                integrated.gender, anilist_char.get("gender")
            )

            # Add AniList favourites to combined favorites
            if anilist_char.get("favourites"):
                mal_favs = integrated.favorites or 0
                anilist_favs = anilist_char.get("favourites", 0)
                integrated.favorites = mal_favs + anilist_favs

            # Add AniList pages and images
            if anilist_char.get("id"):
                integrated.character_pages["anilist"] = (
                    f"https://anilist.co/character/{anilist_char['id']}"
                )
            if anilist_char.get("image", {}).get("large"):
                images.append(anilist_char["image"]["large"])

        # Integrate AniDB data - ADD name variations
        if anidb_match:
            anidb_char = anidb_match.target_char
            integrated.source_count += 1

            # Add AniDB name variation
            name_variations_dict = CharacterNamePreprocessor.add_name_variation(
                name_variations_dict, anidb_char.get("name")
            )

            # Fill missing data from AniDB
            integrated.gender = CharacterNamePreprocessor.fill_missing_field(
                integrated.gender, anidb_char.get("gender")
            )

            # Add AniDB pages
            if anidb_char.get("id"):
                integrated.character_pages["anidb"] = (
                    f"https://anidb.net/character/{anidb_char['id']}"
                )

            # Construct AniDB image URL from 'picture' field
            if anidb_char.get("picture"):
                anidb_image_url = (
                    f"https://cdn.anidb.net/images/main/{anidb_char['picture']}"
                )
                images.append(anidb_image_url)

        # Integrate AnimePlanet data - ADD rich character metadata
        if anime_planet_match:
            ap_char = anime_planet_match.target_char
            integrated.source_count += 1

            # Add AnimePlanet name variation
            name_variations_dict = CharacterNamePreprocessor.add_name_variation(
                name_variations_dict, ap_char.get("name")
            )

            # Fill missing data from AnimePlanet (has rich metadata)
            integrated.gender = CharacterNamePreprocessor.fill_missing_field(
                integrated.gender, ap_char.get("gender")
            )
            integrated.age = CharacterNamePreprocessor.fill_missing_field(
                integrated.age, ap_char.get("age"), convert_to_str=True
            )

            # Extract physical attributes from AnimePlanet
            integrated.hair_color = CharacterNamePreprocessor.fill_missing_field(
                integrated.hair_color, ap_char.get("hair_color")
            )
            integrated.eye_color = CharacterNamePreprocessor.fill_missing_field(
                integrated.eye_color, ap_char.get("eye_color")
            )

            # Extract character traits/tags from AnimePlanet
            if ap_char.get("tags"):
                integrated.character_traits = ap_char.get("tags", [])

            # AnimePlanet URL (character slug)
            ap_url = ap_char.get("url", "")
            if ap_url:
                integrated.character_pages["animeplanet"] = (
                    f"https://www.anime-planet.com{ap_url}"
                )

            # Add AnimePlanet image
            if ap_char.get("image"):
                images.append(ap_char["image"])

        # Finalize integrated data - use dict values to get deduplicated names
        integrated.name_variations = list(name_variations_dict.values())
        integrated.images = images

        # Determine overall confidence based on source count and match quality
        if integrated.source_count >= 2:
            integrated.match_confidence = MatchConfidence.HIGH
        else:
            integrated.match_confidence = MatchConfidence.LOW

        return integrated

    def _extract_voice_actors(self, character: Dict[str, Any]) -> List[Dict[str, str]]:
        """Extract and simplify voice actor data"""
        voice_actors = []

        for va in character.get("voice_actors", []):
            voice_actors.append(
                {
                    "name": va.get("person", {}).get("name", ""),
                    "language": va.get("language", ""),
                }
            )

        return voice_actors


async def process_characters_with_ai_matching(
    jikan_chars: List[Dict[str, Any]],
    anilist_chars: List[Dict[str, Any]],
    anidb_chars: List[Dict[str, Any]],
    anime_planet_chars: Optional[List[Dict[str, Any]]] = None,
    matcher: Optional["AICharacterMatcher"] = None,
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Process and integrate character data from multiple sources using AI-powered matching.
    
    This function runs the ensemble matcher over Jikan, AniList, AniDB, and optionally AnimePlanet character lists, consolidates enriched metadata into the Stage 5 JSON schema, and returns a dictionary with a single "characters" key containing the processed character records. If a reusable AICharacterMatcher instance is supplied it will be reused; otherwise a new matcher is created.
    
    Parameters:
        jikan_chars (List[Dict[str, Any]]): Character entries from Jikan (primary source).
        anilist_chars (List[Dict[str, Any]]): Character entries from AniList.
        anidb_chars (List[Dict[str, Any]]): Character entries from AniDB.
        anime_planet_chars (Optional[List[Dict[str, Any]]]): Character entries from AnimePlanet; may be omitted.
        matcher (Optional[AICharacterMatcher]): An existing AICharacterMatcher to reuse; a new one is created when omitted.
    
    Returns:
        Dict[str, List[Dict[str, Any]]]: A dictionary with a "characters" list where each entry is a dict containing the integrated fields:
            - age, description, eye_color, favorites, gender, hair_color, name, name_native, role
            - character_traits, images, name_variations, nicknames, voice_actors
            - character_pages
            - _match_scores (internal per-source match scoring; may be an empty dict)
    """

    # Reuse matcher if provided, otherwise create new instance
    if matcher is None:
        matcher = AICharacterMatcher()

    # Process all characters with AI matching
    processed_chars = await matcher.match_characters(
        jikan_chars, anilist_chars, anidb_chars, anime_planet_chars
    )

    # Convert to output format
    output_characters = []

    for char in processed_chars:
        output_char = {
            # =====================================================================
            # SCALAR FIELDS (alphabetical)
            # =====================================================================
            "age": char.age,
            "description": char.description,
            "eye_color": char.eye_color,
            "favorites": char.favorites,
            "gender": char.gender,
            "hair_color": char.hair_color,
            "name": char.name,
            "name_native": char.name_native,
            "role": char.role,
            # =====================================================================
            # ARRAY FIELDS (alphabetical)
            # =====================================================================
            "character_traits": char.character_traits,
            "images": char.images,
            "name_variations": char.name_variations,
            "nicknames": char.nicknames,
            "voice_actors": char.voice_actors,
            # =====================================================================
            # OBJECT/DICT FIELDS (alphabetical)
            # =====================================================================
            "character_pages": char.character_pages,
            # Internal field for stage5 to use (not part of final output)
            "_match_scores": char.match_scores if char.match_scores else {},
        }

        output_characters.append(output_char)

    logger.info(
        f"AI character matching complete: {len(output_characters)} characters with confidence distribution:"
    )

    # Log confidence statistics
    confidence_stats: Dict[str, int] = {}
    for char in processed_chars:
        conf = char.match_confidence.value
        confidence_stats[conf] = confidence_stats.get(conf, 0) + 1

    for conf, count in confidence_stats.items():
        logger.info(f"  {conf}: {count} characters")

    return {"characters": output_characters}


if __name__ == "__main__":  # pragma: no cover
    # Example usage
    async def main() -> int:
        """
        Run a sample CLI workflow that invokes the AI character matcher and prints results as JSON.

        Executes a demonstration run using mock character data, prints the matcher output to stdout as formatted JSON, and returns a numeric exit code.

        Returns:
            int: `0` on successful completion, `1` if an unexpected error occurred (and an error message is written to stderr).
        """
        try:
            # Mock data for testing
            jikan_chars = [{"name": "Spike Spiegel", "role": "Main", "mal_id": 1}]
            anilist_chars = [{"name": {"full": "Spike Spiegel"}, "id": 1}]
            anidb_chars = [{"name": "Spike Spiegel", "id": 118}]

            result = await process_characters_with_ai_matching(
                jikan_chars, anilist_chars, anidb_chars
            )

            print(json.dumps(result, indent=2, ensure_ascii=False))
        except KeyboardInterrupt:
            print("\nInterrupted by user", file=sys.stderr)
            return 1
        except Exception:
            logger.exception("Unexpected error in character matcher CLI")
            print("Error: An unexpected error occurred. Check logs for details.", file=sys.stderr)
            return 1
        else:
            return 0

    sys.exit(asyncio.run(main()))